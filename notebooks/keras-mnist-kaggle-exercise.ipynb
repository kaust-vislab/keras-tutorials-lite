{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Exercise with MNIST and Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This exercise complements the topics learned in the prerequisite tutorial [Deep Learning from Pre-Trained Models with Keras](keras-transfer-learning-tutorial.ipynb).\n",
    "\n",
    "This exercise aims to reinforce the previous learnings and prepare participants to apply Keras and Image Classification to their own datasets.  Participants will:\n",
    "\n",
    "* Practice what they've learned from the prerequisite *Deep Learning from Pre-Trained Models with Keras* tutorial,\n",
    "* create their own CNN based image classifier for the MNIST digits dataset,\n",
    "* and finally, submit the classification results from their model to Kaggle for evaluation.\n",
    "\n",
    "Participants are expected to bring their own laptops and sign-up for free online cloud services (e.g., Google Colab, Kaggle).  They may also need to download free, open-source software prior to arriving for the workshop.\n",
    "\n",
    "* Tutorial materials are derived from:\n",
    "  * [PyTorch Tutorials](https://github.com/kaust-vislab/pytorch-tutorials) by David Pugh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Colab\n",
    "\n",
    "In order to run this notebook in [Google Colab](https://colab.research.google.com) you will need a [Google Account](https://accounts.google.com/).  Sign-in to your Google account, if necessary, and then start the notebook.\n",
    "\n",
    "Change Google Colab runtime to use GPU:\n",
    "\n",
    "* Click `Runtime` -> `Change runtime type` menu item\n",
    "* Specify `Hardware accelerator` as `GPU`\n",
    "* Click **[Save]** button\n",
    "\n",
    "The session indicator (toolbar / status ribbon under menu) should briefly appear as `Connecting...`.  When the session restarts, continue with the next cell (specifying TensorFlow version v2.x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load cache_utils.py\n",
    "import pathlib\n",
    "import tensorflow.keras.utils as Kutils\n",
    "\n",
    "def cache_mnist_data():\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\"]:\n",
    "        path = pathlib.Path(\"../datasets/mnist/%s\" % n).absolute()\n",
    "        if not path.is_file():\n",
    "            print(\"missing local dataset file: %s\" % n)\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        try:\n",
    "            data_file_path = Kutils.get_file(n.replace('/','-mnist-'), DATA_URL)\n",
    "            print(\"cached file: %s\" % n)\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)\n",
    "\n",
    "def cache_cifar10_data():\n",
    "    for n in [\"cifar-10.npz\", \"cifar-10-batches-py.tar.gz\"]:\n",
    "        path = pathlib.Path(\"../datasets/cifar10/%s\" % n).absolute()\n",
    "        if not path.is_file():\n",
    "            print(\"missing local dataset file: %s\" % n)\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        try:\n",
    "            data_file_path = Kutils.get_file(n, DATA_URL)\n",
    "            print(\"cached file: %s\" % n)\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)\n",
    "\n",
    "def cache_models():\n",
    "    for n in [\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"]:\n",
    "        path = pathlib.Path(\"../models/%s\" % n).absolute()\n",
    "        if not path.is_file():\n",
    "            print(\"missing local dataset file: %s\" % n)\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        try: \n",
    "            data_file_path = Kutils.get_file(n, DATA_URL, cache_subdir='models')\n",
    "            print(\"cached file: %s\" % n)\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download MNIST with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, then dowload the data using the Keras `load_data()` API by running the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "cache_mnist_data();\n",
    "mnist.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Setup\n",
    "\n",
    "Initialize the Python environment by importing and verifying the modules we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%matplotlib inline` is a magic command that makes *matplotlib* charts and plots appear was outputs in the notebook.\n",
    "\n",
    "`%matplotlib notebook` enables semi-interactive plots that can be enlarged, zoomed, and cropped while the plot is active.  One issue with this option is that new plots appear in the active plot widget, not in the cell where the data was produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load verify_runtime.py\n",
    "# Verify runtime environment\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "print(\"is_colab:\", IS_COLAB)\n",
    "\n",
    "assert tf.__version__ >= \"2.0\", \"TensorFlow version >= 2.0 required.\"\n",
    "print(\"tensorflow_version:\", tf.__version__)\n",
    "\n",
    "assert sys.version_info >= (3, 5), \"Python >= 3.5 required.\"\n",
    "print(\"python_version:\", \"%s.%s.%s-%s\" % (sys.version_info.major, \n",
    "                                          sys.version_info.minor,\n",
    "                                          sys.version_info.micro,\n",
    "                                          sys.version_info.releaselevel\n",
    "                                         ))\n",
    "\n",
    "print(\"executing_eagerly:\", tf.executing_eagerly())\n",
    "\n",
    "try:\n",
    "    __physical_devices = tf.config.list_physical_devices('GPU')\n",
    "except AttributeError:\n",
    "    __physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    \n",
    "if len(__physical_devices) == 0:\n",
    "    print(\"No GPUs available. Expect training to be very slow.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to `Runtime` > `Change runtime` and select a GPU hardware accelerator.\"\n",
    "              \"Then `Save` to restart session.\")\n",
    "else:\n",
    "    print(\"is_built_with_cuda:\", tf.test.is_built_with_cuda())\n",
    "    print(\"gpus_available:\", [d.name for d in __physical_devices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Pre-processing - MNIST\n",
    "\n",
    "The previously acquired MNIST dataset is the essential input needed to train an image classification model. Before using the dataset, there are several preprocessing steps required to load the data, and create the correctly sized training, validation, and testing arrays used as input to the network.\n",
    "\n",
    "The following data preparation steps are needed before they can become inputs to the network:\n",
    "\n",
    "* Cache the downloaded dataset (to assist Keras `load_data()` functionality).\n",
    "* Load the dataset (MNIST is small, and fits in memory).\n",
    "    * Convert from textual CSV files into binary arrays.\n",
    "    * Reshape from (784, 1) to (28, 28,1) â€“ and maybe to (32, 32, 3)\n",
    "* Verify the shape and type of the data, and understand it...\n",
    "* Convert label indices into categorical vectors.\n",
    "* Convert image data from integer to float values, and normalize.\n",
    "  * Verify converted input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Data\n",
    "\n",
    "Make downloaded data available to Keras.  Provide dataset utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache MNIST Datasets\n",
    "cache_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ~/.keras -name \"*mnist*\" -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functionality to provide human-readable labels\n",
    "def mnist_index_label(idx):\n",
    "    return int(idx)\n",
    "\n",
    "def mnist_category_label(cat):\n",
    "    return mnist_index_label(cat.argmax())\n",
    "\n",
    "def mnist_label(v):\n",
    "    return mnist_index_label(v) if np.isscalar(v) or np.size(v) == 1 else mnist_category_label(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "Load data via Keras API.  This loads data into a `numpy` array, and the test examples are labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Backup plan: Run the following cell if the data didn't load via `mnist.load_data` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try secondary data source if the first didn't work\n",
    "try:\n",
    "    print(\"data loaded.\" if type((x_train, y_train, x_test, y_test)) else \"load failed...\")\n",
    "except NameError:\n",
    "    with np.load('../datasets/mnist/mnist.npz') as data:\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "        y_test = data['y_test']\n",
    "    print(\"alternate data load.\" if type((x_train, y_train, x_test, y_test)) else \"failed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train), ',', 'y_train type:', type(y_train))\n",
    "print('x_train dtype:', x_train.dtype, ',', 'y_train dtype:', y_train.dtype)\n",
    "print('x_train shape:', x_train.shape, ',', 'y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape, ',', 'y_test shape:', y_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train (min, max, mean): (%s, %s, %s)' % (x_train.min(), x_train.max(), x_train.mean()))\n",
    "print('y_train (min, max): (%s, %s)' % (y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_image(arr):\n",
    "    if len(arr.shape) == 3 and arr.shape[-1] == 3:\n",
    "        return arr\n",
    "    elif len(arr.shape) == 3 and arr.shape[-1] == 1:\n",
    "        return arr.reshape(arr.shape[0:-1])\n",
    "    elif len(arr.shape) == 1 and np.sqrt(arr.shape[0]) == int(np.sqrt(arr.shape[0])):\n",
    "        return arr.reshape((int(np.sqrt(arr.shape[0])), int(np.sqrt(arr.shape[0]))))\n",
    "    else:\n",
    "        return arr # Give up, let `matplotlib` `imshow` complain about malformed image.\n",
    "\n",
    "def imageset_plot(img_data=None):\n",
    "    (x_imgs, y_imgs) = img_data if img_data else (x_train, y_train)\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_imgs.shape[0]))\n",
    "        plt.title(\"%s\" % (mnist_label(y_imgs[idx])))\n",
    "        plt.imshow(to_image(x_imgs[idx]), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "imageset_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load histogram_utils.py\n",
    "# Histogram utils\n",
    "\n",
    "def histogram_plot(img_data=None):\n",
    "  (x_data, y_data) = img_data if img_data else (x_train, y_train)\n",
    "\n",
    "  fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.hist(y_data, bins = range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "  plt.xticks(range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "  plt.title(\"y histogram\")\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.hist(x_data.flat, bins = range(int(x_data.min()), int(x_data.max() + 2)))\n",
    "  plt.title(\"x histogram\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  hist, bins = np.histogram(y_data, bins = range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "  print('y histogram counts:', hist)\n",
    "\n",
    "def histogram_label_plot(train_img_data=None, test_img_data=None):\n",
    "  (x_train_data, y_train_data) = train_img_data if train_img_data else (x_train, y_train)\n",
    "  (x_test_data, y_test_data) = test_img_data if test_img_data else (x_test, y_test)\n",
    "\n",
    "  x_data_min = int(min(x_train_data.min(), x_test_data.min()))\n",
    "  x_data_max = int(min(x_train_data.max(), x_test_data.max()))\n",
    "  y_data_min = int(min(y_train_data.min(), y_test_data.min()))\n",
    "  y_data_max = int(min(y_train_data.max(), y_test_data.max()))\n",
    "  num_rows = y_data_max - y_data_min + 1\n",
    "  \n",
    "  fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "  plot_num = 1\n",
    "  for lbl in range(y_data_min, y_data_max):\n",
    "    plt.subplot(num_rows, 2 , plot_num)\n",
    "    plt.hist(x_train_data[y_train_data.squeeze() == lbl].flat, bins = range(x_data_min, x_data_max + 2))\n",
    "    plt.title(\"x train histogram - label %s\" % lbl)\n",
    "    plt.subplot(num_rows, 2 , plot_num + 1)\n",
    "    plt.hist(x_test_data[y_test_data.squeeze() == lbl].flat, bins = range(x_data_min, x_data_max + 2))\n",
    "    plt.title(\"x test histogram - label %s\" % lbl)\n",
    "    plot_num += 2\n",
    "\n",
    "  plt.tight_layout(pad=0)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks reasonable: there are sufficient examples for each category (y_train) and the histogram showning mostly black (0) and near-white grayscale (>250) agrees with the examples shown previously.\n",
    "\n",
    "Lets do one more sanity check to ensure that the data distributions are also similar per-category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_label_plot((x_train, y_train), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The per-category histograms also look similar (again, the background dominates the histogram).\n",
    "\n",
    "##### Visualizing training samples using PCA\n",
    "\n",
    "[Principal Components Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) can be used as a visualization tool to see if there are any obvious patterns in the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=40, random_state=_prng)\n",
    "\n",
    "x_train_flat = x_train.reshape(*x_train.shape[:1], -1)\n",
    "y_train_flat = y_train.reshape(y_train.shape[0])\n",
    "print(\"x_train:\", x_train.shape, \"y_train\", y_train.shape)\n",
    "print(\"x_train_flat:\", x_train_flat.shape, \"y_train_flat\", y_train_flat.shape)\n",
    "pca_train_features = pca.fit_transform(x_train_flat, y_train_flat)\n",
    "print(\"pca_train_features:\", pca_train_features.shape)\n",
    "\n",
    "# Sample 10% of the PCA results\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "pca_features = pca_train_features[_idxs]\n",
    "pca_category = y_train_flat[_idxs]\n",
    "print(\"pca_features:\", pca_features.shape, \n",
    "      \"pca_category\", pca_category.shape, \n",
    "      \"min,max category:\", pca_category.min(), pca_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_components_plot(components_, shape_=(32, 32, 3)):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    for i in range(min(40, components_.shape[0])):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        eigen_vect = (components_[i] - np.min(components_[i])) / np.ptp(pca.components_[i])\n",
    "        plt.title('component: %s' % i)\n",
    "        plt.imshow(eigen_vect.reshape(shape_), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_components_plot(pca.components_, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def category_scatter_plot(features, category, title='MNIST'):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    sc = ax.scatter(features[:,0], features[:,1], c=category, alpha=0.4, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def category_scatter3d_plot(features, category, title='MNIST'):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "    mean_feat = np.mean(features, axis=0)\n",
    "    std_feat = np.std(features, axis=0)\n",
    "    min_range = mean_feat - std_feat\n",
    "    max_range = mean_feat + std_feat\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(features[:,0], features[:,1], features[:,2],\n",
    "                    c=category, alpha=0.85, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(2.0 * min_range[0], 2.0 * max_range[0])\n",
    "    ax.set_ylim(2.0 * min_range[1], 2.0 * max_range[1])\n",
    "    ax.set_zlim(2.0 * min_range[2], 2.0 * max_range[2])\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter_plot(pca_features, pca_category, title='MNIST - PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 3D PCA plot works best with `%matplotlib notebook` to enable interactive rotation (enabled at start of session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter3d_plot(pca_features, pca_category, title='MNIST - PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in its original image space appears to *almost* clump into categories, but is not cleanly seperable.\n",
    "\n",
    "##### Visualizing training sample using t-SNE\n",
    "\n",
    "[t-distributed Stochastic Neighbor Embedding (t-SNE)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. For more details on t-SNE including other use cases see this excellent *Toward Data Science* [blog post](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "import sklearn.manifold\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "embedding2_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=2, random_state=_prng))\n",
    "\n",
    "embedding3_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=3, random_state=_prng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the data\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "tsne_features = x_train_flat[_idxs]\n",
    "tsne_category = y_train_flat[_idxs]\n",
    "print(\"tsne_features:\", tsne_features.shape, \n",
    "      \"tsne_category\", tsne_category.shape, \n",
    "      \"min,max category:\", tsne_category.min(), tsne_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform2_tsne_features = embedding2_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform2_tsne_features:\", transform2_tsne_features.shape)\n",
    "for i in range(2):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform2_tsne_features[:,i].min(), \n",
    "          transform2_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter_plot(transform2_tsne_features, tsne_category, title='MNIST - t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE relates the data points (images) according to their closest neighbours.  The *MNIST - t-SNE* plot above shows that the manifold of the original image space can *almost* be well separated into distinct clusters.  Notice the closely related clusters of 9-(blue) & 4-(purple), and 3-(red) & 5-(brown).  When written, those pairs of digit shapes have recognizable similarities to each other too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform3_tsne_features = embedding3_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform3_tsne_features:\", transform3_tsne_features.shape)\n",
    "for i in range(3):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform3_tsne_features[:,i].min(), \n",
    "          transform3_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter3d_plot(transform3_tsne_features, tsne_category, title='MNIST - t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "The data type for the training data is `uint8`, while the input type for the network will be `float32` so the data must be converted.  Also, the data should be normalized, and the labels need to be categorical.  I.e., instead of label existing as 10 different values in a 1-D space, they need to exist as Boolean values in a 10-D space â€” one dimension for each category, and either a 0 or 1 value in each dimension to represent membership in that category.\n",
    "\n",
    "* https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = (y_train.max() - y_train.min()) + 1\n",
    "print('num_classes =', num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape:\", x_train.shape, x_test.shape)\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "print(\"reshape:\", x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train))\n",
    "print('x_train dtype:', x_train.dtype)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('y_train type:', type(y_train))\n",
    "print('y_train dtype:', y_train.dtype)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize accuracy and loss for training and validation.\n",
    "\n",
    "* https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.title('Model accuracy & loss')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    ax1 = fig.add_subplot()\n",
    "    #ax1.set_ylim(0, 1.1 * max(history.history['loss']+history.history['val_loss']))\n",
    "    ax1.set_prop_cycle(color=['green', 'red'])\n",
    "    p1 = ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    p2 = ax1.plot(history.history['val_loss'], label='Test Loss')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0, 1.1 * max(history.history['accuracy']+history.history['val_accuracy']))\n",
    "    ax2.set_prop_cycle(color=['blue', 'orange'])\n",
    "    p3 = ax2.plot(history.history['accuracy'], label='Train Acc')\n",
    "    p4 = ax2.plot(history.history['val_accuracy'], label='Test Acc')\n",
    "\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    pz = p3 + p4 + p1 + p2\n",
    "    plt.legend(pz, [l.get_label() for l in pz], loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    rSym = ''\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict(x_test[idx:idx+1])[0]\n",
    "        if y_test is not None:\n",
    "            rCorrect = True if np.argmax(y_test[idx]) == np.argmax(result) else False\n",
    "            rSym = 'âœ”' if rCorrect else 'âœ˜'\n",
    "            correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, np.argmax(result)))\n",
    "        plt.imshow(x_test[idx][:,:,0], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    if y_test is not None:\n",
    "        print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_classes_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    rSym = ''\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_classes(x_test[idx:idx+1])[0]\n",
    "        if y_test is not None:\n",
    "            rCorrect = True if np.argmax(y_test[idx]) == result else False\n",
    "            rSym = 'âœ”' if rCorrect else 'âœ˜'\n",
    "            correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, result))\n",
    "        plt.imshow(x_test[idx][:,:,0], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    if y_test is not None:\n",
    "        print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_proba_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 2, (2*i) + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_proba(x_test[idx:idx+1])[0] * 100 # prob -> percent\n",
    "        if y_test is not None:\n",
    "            plt.title(\"%s\" % np.argmax(y_test[idx]))\n",
    "        plt.xlabel(\"#%s\" % idx)\n",
    "        plt.imshow(x_test[idx][:,:,0], cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "        ax = plt.subplot(10, 2, (2*i) + 2)\n",
    "        plt.bar(np.arange(len(result)), result, label='%')\n",
    "        plt.xticks(range(0, len(result) + 1))\n",
    "        ax.set_xticklabels(range(10))\n",
    "        plt.title(\"classifier probabilities\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Your Own CNN Classifier Model\n",
    "\n",
    "Create a basic CNN (Convolutional Neural Network) based classifier from scratch.\n",
    "\n",
    "Try and create your own deep learning model to classify the MNIST data. Refer to the prerequisite tutorial and use the [CNN Classifier Model](keras-transfer-learning-tutorial.ipynb#CNN-Classifier-Model) code as a template.  Here are a few ideas to try.\n",
    "\n",
    "1. Add more convolutional layers.\n",
    "2. Add more neurons in each convolutional layer(s).\n",
    "3. Try different activation layers.\n",
    "4. Try using a different optimizer.\n",
    "5. Try tuning the hyper-parameters of your chosen optimizer.\n",
    "6. Train the model for more epochs (but don't overfit!)\n",
    "\n",
    "* https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# TODO: Tune the following hyper-parameters as needed\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "learning_rate = 1e-3\n",
    "decay = 1e-6\n",
    "\n",
    "\n",
    "def create_my_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: Add your model creation code here...\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_my_compiled_model(learning_rate=1e-4, decay=1e-6):\n",
    "    model = create_my_model()\n",
    "\n",
    "    # TODO: Add your `model.compile` code here...\n",
    "   \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_my_compiled_model(learning_rate=learning_rate, decay=decay)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Digit Recognizer Competition\n",
    "\n",
    "Kaggle hosts an MNIST-base [Digit Recognizer competition](https://www.kaggle.com/c/digit-recognizer), complete with a [rolling leaderboard](https://www.kaggle.com/c/digit-recognizer/leaderboard). We will enter this competition using our MNIST image classification network above, and compare its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Kaggle Account\n",
    "\n",
    "#### 1. Register for an account\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "#### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data.\n",
    "\n",
    "* Go to the `Account` tab of your user profile, \n",
    "* and click `Create New API Token` from the API section.  \n",
    "\n",
    "This generates a `kaggle.json` file (with 'username' and 'key' values) to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download MNIST Data\n",
    "\n",
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "##### Download MNIST from Kaggle\n",
    "\n",
    "**Note:** Before attempting to download the competition data you will need to login to your [Kaggle](https://www.kaggle.com) account and accept the rules for this competition.\n",
    "\n",
    "Set your Kaggle username and API key (from the `kaggle.json` file) into the cell below, and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../datasets/mnist/kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for archive in ../datasets/mnist/kaggle/*.zip ; do\n",
    "  unzip -n \"${archive}\" -d ../datasets/mnist/kaggle\n",
    "done\n",
    "find ../datasets/mnist/kaggle -name \"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download MNIST from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist\"\n",
    "    DEST_DIR = pathlib.Path('../datasets/mnist')\n",
    "\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\", \"kaggle/sample_submission.csv\"]:\n",
    "        path = DEST_DIR / n\n",
    "        path.parent.mkdir(exist_ok=True)\n",
    "        if not path.is_file():  # Don't download if file exists\n",
    "            with path.open(mode = 'wb') as f:\n",
    "                response = requests.get(RAW_URL + \"/\" + n)\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()\n",
    "cache_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting to Kaggle\n",
    "\n",
    "Submit your model's predictions to Kaggle, using your previously created Kaggle account, and then see how well your results compare to those of your peers.\n",
    "\n",
    "The Kaggle MNIST dataset is provided in CSV (Comma Separated Values) format; a common textual format for data exchange.  Each line in a CSV file corresponds to a row in the table.  The first line may represent the column lables.\n",
    "\n",
    "Because the files are text, we can view their content with simple shell commands like `cat`, `less`, `head`, or `tail`.  The `head` command shows the first `-n` lines of the file.  We will view the first two lines of each file in the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 2 ../datasets/mnist/kaggle/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 2 ../datasets/mnist/kaggle/test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first line contains the column labels, and that the first column label of the `train.csv` dataset is `label`.  Notice that the `test.csv` file does not have a `label` column.  Finally, note that the second line, which will be the data for a single image has mostly small, positive integer values.\n",
    "\n",
    "[pandas](https://pandas.pydata.org/) is popular Python library for data analysis.  It has many useful features, including efficiently loading CSV data, converting them to Numpy arrays, and pretty-printing large tables in Jupyter notebooks.  Let's take a quick look at our data with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../datasets/mnist/kaggle/train.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../datasets/mnist/kaggle/test.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `pandas` it is easy to determine the dimensions of the two files (shown below table), which will be useful information when specifying which columns are part of the image.\n",
    "\n",
    "#### Load Data\n",
    "\n",
    "Load the Kaggle version of MNIST CSV format data via pandas API and convert into a `numpy` array.  \n",
    "\n",
    "**Note:** There are no 'test labels' for this dataset, because the test accuracy will be evaluated by Kaggle, not by us.  This will have implications for how we validate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_features = pd.read_csv(\"../datasets/mnist/kaggle/train.csv\", \n",
    "                              delimiter=',', skiprows=0, usecols=range(1,785), dtype=np.uint8).to_numpy()\n",
    "_train_labels = pd.read_csv(\"../datasets/mnist/kaggle/train.csv\",\n",
    "                            delimiter = ',', skiprows=0, usecols=[0], dtype=np.uint8).to_numpy()\n",
    "_test_features = pd.read_csv(\"../datasets/mnist/kaggle/test.csv\",\n",
    "                             delimiter=',', skiprows=0, dtype=np.uint8).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well.\n",
    "\n",
    "Is the Kaggle version of MNIST the same, and the test examples are labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_train_features type:', type(_train_features), '_test_features type:', type(_test_features))\n",
    "print('_train_features dtype:', _train_features.dtype, '_test_features dtype:', _test_features.dtype)\n",
    "print('_train_features shape:', _train_features.shape, '_test_features shape:', _test_features.shape)\n",
    "print(_train_features.shape[0], '_train_features samples')\n",
    "print(_test_features.shape[0], '_test_features samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_train_features (min, max, mean): (%s, %s, %s)' % (_train_features.min(),\n",
    "                                                          _train_features.max(),\n",
    "                                                          _train_features.mean()))\n",
    "print('_test_features (min, max, mean): (%s, %s, %s)' % (_test_features.min(),\n",
    "                                                         _test_features.max(),\n",
    "                                                         _test_features.mean()))\n",
    "print('_train_labels (min, max): (%s, %s)' % (_train_labels.min(),\n",
    "                                              _train_labels.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "imageset_plot((_train_features, _train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((_train_features, _train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle MNIST dataset looks very similar to the original dataset we used previously â€“ similar min, max, mean, and appearance â€“ however, there are fewer samples in the dataset (and the test dataset has no labels), and the image data is flattened into a 1-D array of size 784 (we will need to reshape that into a 2-D array to use convolution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "The data type for the training data is `uint8`, while the input type for the network will be `float32` so the data must be converted.  Also, the data should be normalized, and the labels need to be categorical (one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = (_train_labels.max() - _train_labels.min()) + 1\n",
    "print('num_classes =', num_classes)\n",
    "\n",
    "_train_labels = keras.utils.to_categorical(_train_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape:\", _train_features.shape, _test_features.shape)\n",
    "_train_features = _train_features.reshape((_train_features.shape[0], 28, 28, 1))\n",
    "_test_features = _test_features.reshape((_test_features.shape[0], 28, 28, 1))\n",
    "print(\"reshape:\", _train_features.shape, _test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_features = _train_features.astype('float32')\n",
    "_test_features = _test_features.astype('float32')\n",
    "_train_features /= 255\n",
    "_test_features /= 255\n",
    "\n",
    "train_data = (_train_features, _train_labels)\n",
    "test_data = (_test_features, None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_train_features type:', type(_train_features))\n",
    "print('_train_features dtype:', _train_features.dtype)\n",
    "print('_train_features shape:', _train_features.shape)\n",
    "print('_test_features shape:', _test_features.shape)\n",
    "\n",
    "print('_train_labels type:', type(_train_labels))\n",
    "print('_train_labels dtype:', _train_labels.dtype)\n",
    "print('_train_labels shape:', _train_labels.shape)\n",
    "print('_test_labels:', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "We already have an MNIST classifier `model` trained on a larger MNIST dataset.  Could we use that model for the Kaggle MNIST dataset?\n",
    "\n",
    "#### Evaluate previous MNIST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Trained model score is evaluated on training data, not a withheld testing dataset.\n",
    "#       The testing dataset for Kaggle MNIST is hidden, so only the training set is labelled; \n",
    "#       as a result, the accuracy scores are not entirely accurate.\n",
    "\n",
    "scores = model.evaluate(_train_features, _train_labels, verbose=0)\n",
    "print('Train loss:', scores[0])\n",
    "print('Train accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate Kaggle MNIST model\n",
    "\n",
    "The previouse model should perform well.  We can either skip ahead to the submission step, or try to train our model on the Kaggle MNIST version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Modify the following hyper-parameters if you want to change them from previous values\n",
    "\n",
    "#batch_size = 128\n",
    "#epochs = 12\n",
    "#learning_rate = 1e-3\n",
    "#decay = 1e-6\n",
    "\n",
    "# Create a new compiled model from the function you completed previously\n",
    "model_kaggle = create_my_compiled_model(learning_rate=learning_rate, decay=decay)\n",
    "\n",
    "# Train this model on the Kaggle MNIST training data; \n",
    "# there is no test data; use `validation_split` to withhold part of the training data for validation\n",
    "history = model_kaggle.fit(_train_features, _train_labels,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           validation_split=0.1,\n",
    "                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Trained model score is evaluated on training data, not a withheld testing dataset.\n",
    "#       The testing dataset for Kaggle MNIST is hidden, so only the training set is labelled; \n",
    "#       as a result, the accuracy scores are not entirely accurate.\n",
    "\n",
    "scores = model_kaggle.evaluate(_train_features, _train_labels, verbose=0)\n",
    "print('Train loss:', scores[0])\n",
    "print('Train accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model_kaggle, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model_kaggle, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(_test_features)\n",
    "predictions_kaggle = model_kaggle.predict_classes(_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_pred = predictions == predictions_kaggle\n",
    "differs_idxs = np.argwhere((cmp_pred == False)).flatten()\n",
    "print('differences found:', differs_idxs.size, 'showing random: 10')\n",
    "title_format = \"test index:%s model predicts:%s kaggle model predicts:%s\"\n",
    "for i in differs_idxs[np.random.randint(differs_idxs.size, size=10)]:\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title_format % (i, predictions[i], predictions_kaggle[i]))\n",
    "    plt.imshow(to_image(_test_features[i]), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../datasets/mnist/kaggle/sample_submission.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "SUBMISSION_DIR = pathlib.Path('../results/kaggle-submissions')\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "SUBMISSION_FILE = SUBMISSION_DIR / f\"submission-{timestamp}.csv\"\n",
    "\n",
    "number_predictions = predictions_kaggle.shape[0]\n",
    "\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions_kaggle})\n",
    "df.to_csv(str(SUBMISSION_FILE), index=False)\n",
    "\n",
    "pd.read_csv(str(SUBMISSION_FILE), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle\n",
    "\n",
    "Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https://www.kaggle.com/c/digit-recognizer) website and see how well your best model compares to your peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "# Submits the latest `submission-*.csv` file\n",
    "kaggle competitions submit -c digit-recognizer \\\n",
    "  -f \"$(ls ../results/kaggle-submissions/submission-*.csv | tail -n 1)\" \\\n",
    "  -m \"My digit recognizer submission!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Results\n",
    "\n",
    "Check out your results on the Kaggle competition leaderboards for `digit-recognizer`:\n",
    "\n",
    "* [Competitions](https://www.kaggle.com/competitions).\n",
    "* [Leaderboard](https://www.kaggle.com/c/digit-recognizer/leaderboard).\n",
    "\n",
    "Your entry should be highlighted if you're logged in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You are now ready to explore deeper into Keras Image Classification.  Many happy trainings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentor Bios\n",
    "\n",
    "Glendon Holst is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in HPC workflow solutions for deep learning, image processing, and scientific visualization.\n",
    "\n",
    "David R. Pugh is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in Data Science and Machine Learning. David is also a certified Software and Data Carpentry Instructor and Instructor Trainer and is the lead instructor of the Introduction to Data Science Workshop series at KAUST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\n",
    "* http://yann.lecun.com/exdb/mnist/index.html\n",
    "* https://www.kaggle.com/c/digit-recognizer\n",
    "* https://jupyter-notebook.readthedocs.io/en/stable/\n",
    "* https://github.com/kaust-vislab/handson-ml2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
