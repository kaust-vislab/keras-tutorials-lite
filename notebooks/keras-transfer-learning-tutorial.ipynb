{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning from Pre-Trained Models with Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ImageNet, an image recognition benchmark dataset*, helped trigger the modern AI explosion.  In 2012, the AlexNet architecture (a deep convolutional-neural-network) rocked the ImageNet benchmark competition, handily beating the next best entrant.  By 2014, all the leading competitors were deep learning based.  Since then, accuracy scores continued to improve, eventually surpassing human performance.\n",
    "\n",
    "In this hands-on tutorial we will build on this pioneering work to create our own neural-network architecture for image recognition.  Participants will use the elegant Keras deep learning programming interface to build and train TensorFlow models for image classification tasks on the CIFAR-10 / MNIST datasets*.  We will demonstrate the use of transfer learning* (to give our networks a head-start by building on top of existing, ImageNet pre-trained, network layers*), and explore how to improve model performance for standard deep learning pipelines.  We will use cloud-based interactive Jupyter notebooks to work through our explorations step-by-step.  Once participants have successfully trained their custom model we will show them how to submit their model's predictions to Kaggle for scoring*.\n",
    "\n",
    "This tutorial is designed as an introduction to the topic for a general, but technical audience. As a practical introduction, it will focus on tools and their application. Previous ML (Machine Learning) experience is not required; but, previous experience with scripting in Python will help. \n",
    "\n",
    "Participants are expected to bring their own laptops and sign-up for free online cloud services (e.g., Google Colab, Kaggle).  They may also need to download free, open-source software prior to arriving for the workshop.\n",
    "\n",
    "This tutorial assumes some basic knowledge of neural networks. If you’re not already familiar with neural networks, then you can learn the basics concepts behind neural networks at [course.fast.ai](https://course.fast.ai/).\n",
    "\n",
    "* Tutorial materials are derived from:\n",
    "  * [PyTorch Tutorials](https://github.com/kaust-vislab/pytorch-tutorials) by David Pugh.\n",
    "  * [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard, Rachel Thomas, Francisco Ingham.\n",
    "  * [Machine Learning Notebooks](https://github.com/ageron/handson-ml2) (2nd Ed.) by Aurélien Géron.\n",
    "  * *Deep Learning with Python* by François Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks\n",
    "\n",
    "This is a Jupyter Notebook.  It provides a simple, cell-based, IDE for developing and exploring complex ideas via code, visualizations, and documentation.\n",
    "\n",
    "A notebook has two primary types of cells: i) `markdown` cells for textual notes and documentation, such as the one you are reading now, and ii) `code` cells, which contain snippets of code (typically *Python*, but also *bash* scripts) that can be executed.  \n",
    "\n",
    "The currently selected cell appears within a box. A green box indicates that the cell is editable.  Clicking inside a *code* cell makes it selected and editable.  Double-click inside *markdown* cells to edit.\n",
    "\n",
    "Use `Tab` for context-sensitive code-completion assistance when editing Python code in *code* cells.  For example, use code assistance after a `.` seperator to find available object members.  For help documentation, create a new *code* cell, and use commands like `dir(`*module*`)`, `help(`*topic*`)`, `?`*name*, or `??`*function* for user provided *module*, *topic*, variable *name*, or *function* name.  The magic `?` and `??` commands show documentation / source code in a separate pane.\n",
    "\n",
    "Clicking on `[Run]` or pressing `Ctrl-Enter` will execute the contents of a cell.  A *markdown* cell converts to its display version, and a *code* cell runs the code inside.  To the left of a *code* cell is a small text bracket `In [ ]:`.  If the bracket contains an asterix, e.g., `In [*]:`, that cell is currently executing.  Only one cell executes at a time (if multiple cells are *Run*, they are queued up to execute in the order they were run).  When a *code* cell finishes executing, the bracket shows an execution count in the bracket – each *code* cell execution increments the counter and provides a way to determine the order in which codes were executed – e.g., `In [7]` for the seventh cell to complete.  \n",
    "\n",
    "The output produced by a *code* cell appears at the bottom of that cell after it executes.  The output generated by a code cell includes anything printed to the output during execution (e.g., print statements, or thrown errors) and the final value generated by the cell (i.e., not the intermediate values).  The final value is 'pretty printed' by Jupyter.\n",
    "\n",
    "Typically, notebooks are written to be executed in order, from top to bottom.  Behind the scenes, however, each Notebook has a single Python state (the `kernel`), and each *code* cell that executes, modifies that state.  It is possible to modify and re-run earlier cells; however, care must be taken to also re-run any other cells that depend upon the modified one.  List the Python state global variables with the magic command `%wgets`.  The *kernel* can be restarted to a known state, and cell output cleared, if the Python state becomes too confusing to fix manually (choose `Restart & Clear Output` from the Jupyter `Kernel` menu) – this requires running each *code* cell again.\n",
    "\n",
    "Complete user documentation is available at [jupyter-notebook.readthedocs.io](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface). <br/>\n",
    "Many helpful tips and techniques from [28 Jupyter Notebook Tips, Tricks, and Shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Kaggle Account\n",
    "\n",
    "#### 1. Register for an account\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "#### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data.\n",
    "\n",
    "* Go to the `Account` tab of your user profile, \n",
    "* and click `Create New API Token` from the API section.  \n",
    "\n",
    "This generates a `kaggle.json` file (with 'username' and 'key' values) to download.\n",
    "\n",
    "\n",
    "### Setup Colab\n",
    "\n",
    "In order to run this notebook in [Google Colab](https://colab.research.google.com) you will need a [Google Account](https://accounts.google.com/).  Sign-in to your Google account, if necessary, and then start the notebook.\n",
    "\n",
    "Change Google Colab runtime to use GPU:\n",
    "\n",
    "* Click `Runtime` -> `Change runtime type` menu item\n",
    "* Specify `Hardware accelerator` as `GPU`\n",
    "* Click **[Save]** button\n",
    "\n",
    "The session indicator (toolbar / status ribbon under menu) should briefly appear as `Connecting...`.  When the session restarts, continue with the next cell (specifying TensorFlow version v2.x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "There are two image datasets ([CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) and [MNIST](http://yann.lecun.com/exdb/mnist/index.html)) which these tutorial / exercise notebooks use.\n",
    "\n",
    "These datasets are available from a variety of sources, including this repository – depending on how the notebook was launched (e.g., Git+LFS / Binder contains entire repository, Google Colab only contains the notebook).\n",
    "\n",
    "Because data is the fundamental fuel for deep learning, we need to ensure the required datasets for this tutorial are available to the current notebook session.  The following steps will ensure the data is already available (or downloaded), and cached where Keras can find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow.keras.utils as Kutils\n",
    "\n",
    "def cache_mnist_data():\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\"]:\n",
    "        path = pathlib.Path(\"../datasets/mnist/%s\" % n).absolute()\n",
    "        if not path.is_file():\n",
    "            print(\"missing local dataset file: %s\" % n)\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        try:\n",
    "            data_file_path = Kutils.get_file(n.replace('/','-mnist-'), DATA_URL)\n",
    "            print(\"cached file: %s\" % n)\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)\n",
    "\n",
    "def cache_cifar10_data():\n",
    "    for n in [\"cifar-10.npz\", \"cifar-10-batches-py.tar.gz\"]:\n",
    "        path = pathlib.Path(\"../datasets/cifar10/%s\" % n).absolute()\n",
    "        if not path.is_file():\n",
    "            print(\"missing local dataset file: %s\" % n)\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        try:\n",
    "            data_file_path = Kutils.get_file(n, DATA_URL)\n",
    "            print(\"cached file: %s\" % n)\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)\n",
    "\n",
    "def cache_models():\n",
    "    for n in [\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"]:\n",
    "        path = pathlib.Path(\"../models/%s\" % n).absolute()\n",
    "        if not path.is_file():\n",
    "            print(\"missing local dataset file: %s\" % n)\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        try: \n",
    "            data_file_path = Kutils.get_file(n, DATA_URL, cache_subdir='models')\n",
    "            print(\"cached file: %s\" % n)\n",
    "        except (FileNotFoundError, ValueError, Exception) as e:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions below and run just the appropriate cells needed to acquire the required datasets:\n",
    "\n",
    "*Note*: Several different methods are provided to acquire these essential datasets.  These are to ensure availability under a variety of contingencies. You don't need to run all the download steps – just until the dataset is acquired. In some environments, these datasets are already provided (so no further efforts are required – skip ahead to the [Tutorial](#Tutorial))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "##### Download MNIST from Kaggle\n",
    "\n",
    "**Note:** Before attempting to download the competition data you will need to login to your [Kaggle](https://www.kaggle.com) account and accept the rules for this competition.\n",
    "\n",
    "Set your Kaggle username and API key (from the `kaggle.json` file) into the cell below, and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../datasets/mnist/kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for archive in ../datasets/mnist/kaggle/*.zip ; do\n",
    "  unzip -n \"${archive}\" -d ../datasets/mnist/kaggle\n",
    "done\n",
    "find ../datasets/mnist/kaggle -name \"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download MNIST from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist\"\n",
    "    DEST_DIR = pathlib.Path('../datasets/mnist')\n",
    "\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\", \"kaggle/sample_submission.csv\"]:\n",
    "        path = DEST_DIR / n\n",
    "        path.parent.mkdir(exist_ok=True)\n",
    "        if not path.is_file():  # Don't download if file exists\n",
    "            with path.open(mode = 'wb') as f:\n",
    "                response = requests.get(RAW_URL + \"/\" + n)\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()\n",
    "cache_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download MNIST with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "cache_mnist_data()\n",
    "mnist.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download CIFAR10 Data\n",
    "\n",
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "##### Download CIFAR10 from Kaggle\n",
    "\n",
    "**Note:** Before attempting to download the competition data you will need to login to your [Kaggle](https://www.kaggle.com) account.\n",
    "\n",
    "Set your Kaggle username and API key (from the `kaggle.json` file) into the cell below, and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle datasets download guesejustin/cifar10-keras-files-cifar10load-data -p ../datasets/cifar10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for archive in ../datasets/cifar10/*.zip ; do\n",
    "  unzip -n \"${archive}\" -d ../datasets/cifar10\n",
    "done\n",
    "find ../datasets/cifar10 -name \"*.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download CIFAR10 with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "cache_cifar10_data()\n",
    "cifar10.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "### Setup\n",
    "\n",
    "Initialize the Python environment by importing and verifying the modules we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%matplotlib inline` is a magic command that makes *matplotlib* charts and plots appear was outputs in the notebook.\n",
    "\n",
    "`%matplotlib notebook` enables semi-interactive plots that can be enlarged, zoomed, and cropped while the plot is active.  One issue with this option is that new plots appear in the active plot widget, not in the cell where the data was produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the runtime environment to ensure it can run this notebook.  If there is an `Exception`, or if there are no GPUs, you will need to run this notebook in a more capable environment (see `README.md`, or ask instructor for additional help)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify runtime environment\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "print(\"is_colab:\", IS_COLAB)\n",
    "\n",
    "assert tf.__version__ >= \"2.0\", \"TensorFlow version >= 2.0 required.\"\n",
    "print(\"tensorflow_version:\", tf.__version__)\n",
    "\n",
    "assert sys.version_info >= (3, 5), \"Python >= 3.5 required.\"\n",
    "print(\"python_version:\", \"%s.%s.%s-%s\" % (sys.version_info.major, \n",
    "                                          sys.version_info.minor,\n",
    "                                          sys.version_info.micro,\n",
    "                                          sys.version_info.releaselevel\n",
    "                                         ))\n",
    "\n",
    "print(\"executing_eagerly:\", tf.executing_eagerly())\n",
    "\n",
    "try:\n",
    "    __physical_devices = tf.config.list_physical_devices('GPU')\n",
    "except AttributeError:\n",
    "    __physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    \n",
    "if len(__physical_devices) == 0:\n",
    "    print(\"No GPUs available. Expect training to be very slow.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to `Runtime` > `Change runtime` and select a GPU hardware accelerator.\"\n",
    "              \"Then `Save` to restart session.\")\n",
    "else:\n",
    "    print(\"is_built_with_cuda:\", tf.test.is_built_with_cuda())\n",
    "    print(\"gpus_available:\", [d.name for d in __physical_devices])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 - Dataset Processing\n",
    "\n",
    "The previously acquired CIFAR10 dataset is the essential input needed to train an image classification model. Before using the dataset, there are several preprocessing steps required to load the data, and create the correctly sized training, validation, and testing arrays used as input to the network.\n",
    "\n",
    "The following data preparation steps are needed before they can become inputs to the network:\n",
    "\n",
    "* Cache the downloaded dataset (to use Keras `load_data()` functionality).\n",
    "* Load the dataset (CIFAR10 is small, and fits into a `numpy` array).\n",
    "* Verify the shape and type of the data, and understand it...\n",
    "* Convert label indices into categorical vectors.\n",
    "* Convert image data from integer to float values, and normalize.\n",
    "  * Verify converted input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Data\n",
    "\n",
    "Make downloaded data available to Keras (and check if it's really there).  Provide dataset utility functions.\n",
    "\n",
    "__Note__: Ignore 'FAILED' messages below _if_ at least one of the `cifar-10*` datasets is found; then choose the appropriate load cell in the 'Load Data' section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache CIFAR10 Datasets\n",
    "cache_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ~/.keras -name \"cifar-10*\" -type f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper function assist with managing the three label representations we will encounter:\n",
    "\n",
    "* label index: a number representing a class\n",
    "* label names: a *human readable* text representation of a class\n",
    "* category vector: a vector space to represent the categories\n",
    "\n",
    "The label index `1` represents an `automobile`, and `2` represents a `bird`; but, `1.5` doesn't make a `bird-mobile`.  We need a representation where each dimension is a continuum of that feature.  There are 10 distinct categories, so we encode them as a 10-dimensional vector space, where the i-th dimension represents the i-th class.  An `automobile` becomes `[0,1,0,0,0,0,0,0,0,0]`, a `bird` becomes `[0,0,1,0,0,0,0,0,0,0]` (these are called *one-hot encodings*), and a `bird-mobile` (which we couldn't represent previously) can be encoded as `[0,0.5,0.5,0,0,0,0,0,0,0]`.\n",
    "\n",
    "**Note:** We already know how our dataset is represented.  Typically, one would load the data first, analyse the class representation, and then write the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functionality to provide human-readable labels\n",
    "cifar10_label_names = ['airplane', 'automobile', \n",
    "                       'bird', 'cat', 'deer', 'dog', 'frog', 'horse', \n",
    "                       'ship', 'truck']\n",
    "\n",
    "def cifar10_index_label(idx):\n",
    "    return cifar10_label_names[int(idx)]\n",
    "\n",
    "def cifar10_category_label(cat):\n",
    "    return cifar10_index_label(cat.argmax())\n",
    "\n",
    "def cifar10_label(v):\n",
    "    return cifar10_index_label(v) if np.isscalar(v) or np.size(v) == 1 else cifar10_category_label(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "Datasets for classification require two parts: i) the input data (`x` in our nomenclature), and ii) the labels (`y`).  Classifiction takes an `x` as input, and returns a `y` (the class) as output.\n",
    "\n",
    "When training a model from a dataset (called the `train`ing dataset), it is important to keep some of the data aside (called the `test` set).  If we didn't, the model could just memorize the data without learning a generalization that would apply to novel related data.  The `test` set is used to evaluate the typical real performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Backup plan: Run the following cell if the data didn't load via `cifar10.load_data` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try secondary data source if the first didn't work\n",
    "try:\n",
    "    print(\"data loaded.\" if type((x_train, y_train, x_test, y_test)) else \"load failed...\")\n",
    "except NameError:\n",
    "    with np.load('../datasets/cifar10/cifar-10.npz') as data:\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "        y_test = data['y_test']\n",
    "    print(\"alternate data load.\" if type((x_train, y_train, x_test, y_test)) else \"failed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train), ',', 'y_train type:', type(y_train))\n",
    "print('x_train dtype:', x_train.dtype, ',', 'y_train dtype:', y_train.dtype)\n",
    "print('x_train shape:', x_train.shape, ',', 'y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape, ',', 'y_test shape:', y_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train (min, max, mean): (%s, %s, %s)' % (x_train.min(), x_train.max(), x_train.mean()))\n",
    "print('y_train (min, max): (%s, %s)' % (y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data is stored in Numpy arrays.\n",
    "* The datatype for both input data and labels is a small unsigned int.  They represent different things though.  The input data represents pixel value, the labels represent the category.\n",
    "* There are 50000 training data samples, and 10000 testing samples.\n",
    "* Each input sample is a colour images of 32x32 pixels, with 3 channels of colour (RGB), for a total size of 3072 bytes.  Each label sample is a single byte.\n",
    "  * A 32x32 pixel, 3-channel colour image (2-D) can be represented as a point in a 3072 dimensional vector space.\n",
    "* We can see that pixel values range between 0-255 (that is the range of `uint8`) and the mean value is close to the middle.  The label values range between 0-9, which corresponds to the 10 categories the labels represent.\n",
    "\n",
    "Lets explore the dataset visually, looking at some actual images, and get a statistical overview of the data.\n",
    "\n",
    "Most of the code in the plotting function below is there to tweak the appearance of the output.  The key functionality comes from `matplotlib` functions `imshow` and `hist`, and `numpy` function `histogram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cifar10_imageset_plot(img_data=None):\n",
    "    (x_imgs, y_imgs) = img_data if img_data else (x_train, y_train)\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_imgs.shape[0]))\n",
    "        plt.title(cifar10_label(y_imgs[idx]))\n",
    "        plt.imshow(x_imgs[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "cifar10_imageset_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def histogram_plot(img_data=None):\n",
    "    (x_data, y_data) = img_data if img_data else (x_train, y_train)\n",
    "    \n",
    "    hist, bins = np.histogram(y_data, bins = range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(y_data, bins = range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "    plt.xticks(range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "    plt.title(\"y histogram\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(x_data.flat, bins = range(int(x_data.min()), int(x_data.max() + 2)))\n",
    "    plt.title(\"x histogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('y histogram counts:', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks reasonable: there are sufficient examples for each category (y_train) and a near-normal distribution of pixel values that appears similar in both the train and test datasets.\n",
    "\n",
    "The next aspect of the input data to grapple with is how the input vector space corresponds with the output category space.  Is the correspondence simple, e.g., distances in the input space relate to distances in the output space; or, more complex.  \n",
    "\n",
    "##### Visualizing training samples using PCA\n",
    "\n",
    "[Principal Components Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) can be used as a visualization tool to see if there are any obvious patterns in the training samples.\n",
    "\n",
    "PCA re-represents the input data by changing the basis vectors that represent them.  These new orthonormal basis vectors (eigen vectors) represent variance in the data (ordered from largest to smallest).  Projecting the data samples onto the first few (2 or 3) dimensions will let us see the data with the biggest differences accounted for.\n",
    "\n",
    "The following cell uses `scikit-learn` to calculate PCA eigen vectors for a random subset of the data (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=40, random_state=_prng)\n",
    "\n",
    "x_train_flat = x_train.reshape(*x_train.shape[:1], -1)\n",
    "y_train_flat = y_train.reshape(y_train.shape[0])\n",
    "print(\"x_train:\", x_train.shape, \"y_train\", y_train.shape)\n",
    "print(\"x_train_flat:\", x_train_flat.shape, \"y_train_flat\", y_train_flat.shape)\n",
    "pca_train_features = pca.fit_transform(x_train_flat, y_train_flat)\n",
    "print(\"pca_train_features:\", pca_train_features.shape)\n",
    "\n",
    "# Sample 10% of the PCA results\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "pca_features = pca_train_features[_idxs]\n",
    "pca_category = y_train_flat[_idxs]\n",
    "print(\"pca_features:\", pca_features.shape, \n",
    "      \"pca_category\", pca_category.shape, \n",
    "      \"min,max category:\", pca_category.min(), pca_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_components_plot(components_, shape_=(32, 32, 3)):\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    for i in range(min(40, components_.shape[0])):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        eigen_vect = (components_[i] - np.min(components_[i])) / np.ptp(pca.components_[i])\n",
    "        plt.title('component: %s' % i)\n",
    "        plt.imshow(eigen_vect.reshape(shape_), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the new eigen vector basis functions suggested by the PCA analysis.  Any image in our dataset can be created as a linear combination of these basis vectors.  At a guess, the most prevalent feature of the dataset is that there is something at the centre of the image that is distinct from the background (components 0 & 2) and there is often a difference between 'land' & 'sky' (component 1) – compare with the sample images shown previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_components_plot(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are 2D and 3D scatter plot functions that colour the points by their labels (so we can see if any 'clumps' of points correspond to actual categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def category_scatter_plot(features, category, title='CIFAR10'):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    sc = ax.scatter(features[:,0], features[:,1], c=category, alpha=0.4, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def category_scatter3d_plot(features, category, title='CIFAR10'):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "    mean_feat = np.mean(features, axis=0)\n",
    "    std_feat = np.std(features, axis=0)\n",
    "    min_range = mean_feat - std_feat\n",
    "    max_range = mean_feat + std_feat\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(features[:,0], features[:,1], features[:,2],\n",
    "                    c=category, alpha=0.85, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(2.0 * min_range[0], 2.0 * max_range[0])\n",
    "    ax.set_ylim(2.0 * min_range[1], 2.0 * max_range[1])\n",
    "    ax.set_zlim(2.0 * min_range[2], 2.0 * max_range[2])\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter_plot(pca_features, pca_category, title='CIFAR10 - PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 3D PCA plot works best with `%matplotlib notebook` to enable interactive rotation (enabled at start of session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter3d_plot(pca_features, pca_category, title='CIFAR10 - PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in its original image space does not appear to cluster into corresponding categories.\n",
    "\n",
    "##### Visualizing training sample using t-SNE\n",
    "\n",
    "[t-distributed Stochastic Neighbor Embedding (t-SNE)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. For more details on t-SNE including other use cases see this excellent *Toward Data Science* [blog post](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1).\n",
    "\n",
    "Informally, t-SNE is preserving the local neighbourhood of data points to help uncover the manifold on which the data lies.  For example, a flat piece of paper with two coloured (e.g., red and blue) regions would be a simple manifold to characterize in 3D space; but, if the paper is crumpled up, it becomes very hard to characterize in the original 3D space (blue and red regions could be very close in this representational space) – instead, by following the cumpled paper (manifold) we would recover the fact that blue and red regions are really very distant, and not nearby at all.\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples.\n",
    "\n",
    "* [An Introduction to t-SNE with Python Example](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "import sklearn.manifold\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "embedding2_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=2, random_state=_prng))\n",
    "\n",
    "embedding3_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=3, random_state=_prng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the data\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "tsne_features = x_train_flat[_idxs]\n",
    "tsne_category = y_train_flat[_idxs]\n",
    "print(\"tsne_features:\", tsne_features.shape, \n",
    "      \"tsne_category\", tsne_category.shape, \n",
    "      \"min,max category:\", tsne_category.min(), tsne_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform2_tsne_features = embedding2_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform2_tsne_features:\", transform2_tsne_features.shape)\n",
    "for i in range(2):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform2_tsne_features[:,i].min(), \n",
    "          transform2_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter_plot(transform2_tsne_features, tsne_category, title='CIFAR10 - t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Skip this step during the tutorial, it will take too long to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       extremely lengthy operation, be prepared to wait... and wait...\n",
    "\n",
    "transform3_tsne_features = embedding3_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform3_tsne_features:\", transform3_tsne_features.shape)\n",
    "for i in range(3):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform3_tsne_features[:,i].min(), \n",
    "          transform3_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter3d_plot(transform3_tsne_features, tsne_category, title='CIFAR10 - t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE relates the data points (images) according to their closest neighbours.  Hints of underlying categories appear; but are not cleanly seperable into the original categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "The data type for the training data is `uint8`, while the input type for the network will be `float32` so the data must be converted.  Also, the labels need to be categorical, or *one-hot encoded*, as discussed previously.  Keras provides utility functions to convert labels to categories (`to_categorical`), and `numpy` makes it easy to perform operations over entire arrays.\n",
    "\n",
    "* https://keras.io/examples/cifar10_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = (y_train.max() - y_train.min()) + 1\n",
    "print('num_classes =', num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data conversion, notice that the datatypes are `float32`, the input `x` data shapes are the same; but, the `y` classification labels are now 10-dimensional, instead of scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train))\n",
    "print('x_train dtype:', x_train.dtype)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('y_train type:', type(y_train))\n",
    "print('y_train dtype:', y_train.dtype)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Pre-Trained Network\n",
    "\n",
    "Download an *ImageNet* pretrained VGG16 network[<sup>1</sup>](#fn1), sans classification layer, shaped for 32x32px colour images<sup>[*](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5)</sup> (the smallest supported size).  This image-feature detection network is an example of a deep CNN (Convolutional Neural Network).\n",
    "\n",
    "**Note:** The network must be fixed – it was already trained on a very large dataset, so training it on our smaller dataset would result in it un-learning valuable generic features.\n",
    "\n",
    "<span id=\"fn1\"><sup>[1]</sup> *Very Deep Convolutional Networks for Large-Scale Image Recognition** by Karen Simonyan and Andrew Zisserman, [arXiv (2014)](https://arxiv.org/abs/1409.1556).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "conv_base.trainable = False\n",
    "keras.utils.plot_model(conv_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary shows the layers, starting from the InputLayer and proceeding through Conv2D convolutional layers, which are then collected at MaxPooling2D layers.\n",
    "\n",
    "A convolutional kernel is a small matrix that looks for a specific, localized, pattern on its inputs.  This pattern is called a `feature`.  The kernel is applied at each location on the input image, and the output is another image – a feature image – that represent the strength of that feature at the given location.  \n",
    "\n",
    "Because the inputs to convolution are images, and the outputs are also images – but transformed into a different feature space – it is possible to stack many convolutional layers on top of each other.\n",
    "\n",
    "A feature image can be reduced in size with a MaxPooling2D layer.  This layer 'pools' an `MxN` region to a single value, taking the largest value from the 'pool'.  The 'Max' in 'MaxPooling' is keeping the *best* evidence for that feature, found in the original region.\n",
    "\n",
    "The InputLayer shape and data type should match with the input data:\n",
    "\n",
    "*Note:* The first dimension of the shape will differ; the input layer has `None` to indicate it accepts a batch sized collection of arrays of the remaining shape.  The input data shape will indicate, in that first axis, how many samples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input layer shape:\", conv_base.layers[0].input.shape)\n",
    "print(\"input layer dtype:\", conv_base.layers[0].input.dtype)\n",
    "print(\"input layer type:\", type(conv_base.layers[0].input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input data shape:\", x_train.shape)\n",
    "print(\"input data dtype:\", x_train.dtype)\n",
    "print(\"input data type:\", type(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Convolutional Layers\n",
    "\n",
    "The following are visualization functions (and helpers) for understanding what the convolutional layers in a network have learned.\n",
    "\n",
    "We may ask questions about each convolutional kernal in a convolutional layer:\n",
    "\n",
    "* What local features is the kernel looking for: `visualize_conv_layer_weights`\n",
    "* For a given input image, what feature image will the kernal produce: `visualize_conv_layer_output`\n",
    "* What input image makes the kernel respond most strongly: `visualize_conv_layer_response`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_image_plot(img_data=None, image_index=None):\n",
    "    (x_imgs, y_imgs) = img_data if img_data else (x_train, y_train)\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = int(random.uniform(0, x_imgs.shape[0]))\n",
    "\n",
    "    plt.imshow(x_imgs[image_index], cmap='gray')\n",
    "    plt.title(\"%s\" % cifar10_label(y_imgs[image_index]))\n",
    "    plt.xlabel(\"#%s\" % image_index)\n",
    "    plt.show()\n",
    "    \n",
    "    return image_index\n",
    "\n",
    "def get_model_layer(model, layer_name):\n",
    "    if type(layer_name) == str:\n",
    "        layer = model.get_layer(layer_name)\n",
    "    else:\n",
    "        m = model\n",
    "        for ln in layer_name:\n",
    "            model = m\n",
    "            m = m.get_layer(ln)\n",
    "        layer = m\n",
    "    return (model, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_weights(model, layer_name):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_weights = layer.weights[0]\n",
    "\n",
    "    max_size = layer_weights.shape[3]\n",
    "    col_size = 12\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_weights.shape,\n",
    "           layer_weights.shape[0], layer_weights.shape[1],\n",
    "           max_size))\n",
    "\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                ax[row][col].imshow(layer_weights[:, :, 0, idx], cmap='gray')\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_output(model, layer_name, image_index=None):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_output = layer.output\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = cifar10_image_plot()\n",
    "        \n",
    "    intermediate_model = keras.models.Model(inputs = model.input, outputs=layer_output) \n",
    "    intermediate_prediction = intermediate_model.predict(x_train[image_index].reshape(1,32,32,3))\n",
    "  \n",
    "    max_size = layer_output.shape[3]\n",
    "    col_size = 10\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_output.shape,\n",
    "           layer_output.shape[1], layer_output.shape[2],\n",
    "           max_size))\n",
    "    \n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                ax[row][col].imshow(intermediate_prediction[0, :, :, idx], cmap='gray')\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def process_image(x):\n",
    "    epsilon = 1e-5\n",
    "    # Normalizes the tensor: centers on 0, ensures that std is 0.1 Clips to [0, 1]\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + epsilon)\n",
    "    x *= 0.1\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_response_pattern(model, conv_layer_output, filter_index=0):\n",
    "    #step_size = 1.0\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    img_tensor = tf.Variable(tf.random.uniform((1, 32, 32, 3)) * 20 + 128.0, trainable=True)\n",
    "\n",
    "    response_model = keras.models.Model([model.inputs], [conv_layer_output])\n",
    "\n",
    "    for i in range(40):\n",
    "        with tf.GradientTape() as gtape:\n",
    "            layer_output = response_model(img_tensor)\n",
    "            loss = K.mean(layer_output[0, :, :, filter_index])\n",
    "            grads = gtape.gradient(loss, img_tensor)\n",
    "            grads /= (K.sqrt(K.mean(K.square(grads))) + epsilon)\n",
    "        img_tensor = tf.Variable(tf.add(img_tensor, grads))\n",
    "\n",
    "    img = np.array(img_tensor[0])\n",
    "    return process_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_response(model, layer_name):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_output = layer.output\n",
    "    \n",
    "    max_size = layer_output.shape[3]\n",
    "    col_size = 12\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_output.shape,\n",
    "           layer_output.shape[1], layer_output.shape[2],\n",
    "           max_size))\n",
    "    \n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                img = generate_response_pattern(model, layer_output, idx)\n",
    "                ax[row][col].imshow(img, cmap='gray')\n",
    "                ax[row][col].set_title(\"%s\" % idx)\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the the first 4 convolution layers, we see that:\n",
    "\n",
    "* All the kernels are 3x3 (i.e., 9 elements each)\n",
    "* Layers 1 & 2 have 64 kernels each (64 different possible features)\n",
    "* Layers 3 & 4 have 128 kernels each (128 different possible features)\n",
    "* Light pixels indicate preference for an activated pixel\n",
    "* Dark pixels indicate preference for an inactive pixel\n",
    "* The kernels seem to represent edges and lines at various angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_weights(conv_base, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given input image, show the corresponding feature image. At the lower level layers (e.g., first Conv2D layer), the feature images seem to capture concepts like 'edges' or maybe 'solid colour'?\n",
    "\n",
    "At higher layers, the size of the feature images decrease because of the MaxPooling.  They also appear more abstract – harder to visually recognize than the original image – however, the features are spatially related to the original image (e.g., if there is a white/high value in the lower-left corner of the feature image, then somewhere on the lower-left corner of the original image, there exists pixels that the network is confident represent the feature in question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_index = cifar10_image_plot()\n",
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:7]:\n",
    "    visualize_conv_layer_output(conv_base, n, image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows which input images cause the greatest response from the convolution kernels.  At lower layers, we see many simple 'wave' textures showing that these kernals like to see edges at particular angles.  At lower-middle layers, the paterns show larger scale and more complexity (like dots and curves); but, still lots of angled edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_response(conv_base, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patterns in the higher levels can get even more complex; but, some of them don't seem to encode for anything but noise.  Maybe these could be pruned to make a smaller network...\n",
    "\n",
    "**Note:** Skip this step during the tutorial, it will take too long to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Visualize mid to higher level convolutional layers; \n",
    "#       lengthy operation, be prepared to wait...\n",
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][4:]:\n",
    "    visualize_conv_layer_response(conv_base, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Base + Classifier Model\n",
    "\n",
    "Create a simple model that has the pre-trained CNN (Convolutional Neural Network) as a base, and adds a basic classifier on top.\n",
    "\n",
    "The new layer types are Flatten, Dense, Dropout, and Activation.\n",
    "\n",
    "The Flatten layer reshapes the input dimensions (2D + 1 channel) into a single dimension.\n",
    "\n",
    "The Dense(x) layer is a layer of (`x`) neurons (represented as a flat 1D array) connected to a flat input.  The size of the input and outputs do not need to match.\n",
    "\n",
    "The Dropout(x) layer withholds a random fraction (`x`) of the input neurons from training during each batch of data.  This limits the ability of the network to `overfit` on the training data (i.e., memorize training data, rather than  learn generalizable rules).\n",
    "\n",
    "Activation is an essential part of (or addition to) each layer.  Layers like Dense are simply linear functions (weighted sums + a bias).  Without a non-linear component, the network could not learn a non-linear function.  Activations like 'relu' (Rectified Linear Unit), 'tanh', or 'sigmoid' are functions to introduce a non-linearity.  They also clamp output values within known ranges.\n",
    "\n",
    "The 'softmax' activation is used to produce probability distributions over multiple categories.\n",
    "\n",
    "This example uses the Sequential API to build the final network.\n",
    "\n",
    "* [Activation Functions in Neural Networks](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def create_cnnbase_classifier_model(conv_base=None):\n",
    "    if not conv_base:\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        conv_base.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our model *model_transfer_cnn* by calling the creation function *create_cnnbase_classifier_model* above.\n",
    "\n",
    "Notice the split of total parameters (\\~15 million) between trainable (\\~0.3 million for our classifier) and non-trainable (\\~14.7 million for the pre-trained CNN).\n",
    "\n",
    "Note also that the final Dense layer squeezes the network down to the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_transfer_cnn = create_cnnbase_classifier_model(conv_base)\n",
    "model_transfer_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Training a model typically involves setting relevant hyperparameters that control aspects of the training process.  Common hyperparameters include:\n",
    "\n",
    "* `epochs`: The number of training passes through the entire dataset.  The number of epochs depends upon the complexity of the dataset, and how effectively the network architecture of the model can learn it.  If the value is too small, the model accuracy will be low.  If the value is too big, then the training will take too long for no additional benefit, as the model accuracy will plateau.\n",
    "* `batch_size`: The number of samples to train during each step. The number should be set so that the GPU memory and compute are well utilized.  The `learning_rate` needs to be set accordingly.\n",
    "* `learning_rate`:  The step-size to update model weights during the training update phase (backpropagation).  Too small, and learning takes too long.  Too large, and we may step over the minima we are trying to find.  The learning rate can be increased as the batch sizes increases (with some caveats), on the assumption that with more data in a larger batch, the error gradient will be more accurate, so therefore, we can take a larger step.\n",
    "* `decay`: Used by some optimizers to decrease the `learning_rate` over time, on the assumption that as we get closer to our goal, we should focus on smaller refinement steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs to be compiled prior to use.  This step enables the model to train efficiently on the GPU device.\n",
    "\n",
    "This step also specifies the loss functions, accuracy metrics, learning strategy (optimizers), and more.\n",
    "\n",
    "Our `loss` is *categorical_crossentropy* because we are doing multi-category classification.\n",
    "\n",
    "We use an RMSprop optimizer, which is a varient of standard gradient descent optimizers that also includes momentum.  Momentum is used to speed up learning in directions where it has been making more progress.\n",
    "\n",
    "* [A Look at Gradient Descent and RMSprop Optimizers](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)\n",
    "* [Understanding RMSprop — faster neural network learning](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model_transfer_cnn.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `fit` function trains the network, and returns a history of training and testing accuracy.\n",
    "\n",
    "*Note:* Because we already have a test dataset, and we are not validating our hyperparameters, we will use the test dataset for validation.  We could have also reserved a fraction of the training data to use for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model_transfer_cnn.fit(x_train, y_train,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(x_test, y_test),\n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize accuracy and loss for training and validation.\n",
    "\n",
    "* https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.title('Model accuracy & loss')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    ax1 = fig.add_subplot()\n",
    "    #ax1.set_ylim(0, 1.1 * max(history.history['loss']+history.history['val_loss']))\n",
    "    ax1.set_prop_cycle(color=['green', 'red'])\n",
    "    p1 = ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    p2 = ax1.plot(history.history['val_loss'], label='Test Loss')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0, 1.1 * max(history.history['accuracy']+history.history['val_accuracy']))\n",
    "    ax2.set_prop_cycle(color=['blue', 'orange'])\n",
    "    p3 = ax2.plot(history.history['accuracy'], label='Train Acc')\n",
    "    p4 = ax2.plot(history.history['val_accuracy'], label='Test Acc')\n",
    "\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    pz = p3 + p4 + p1 + p2\n",
    "    plt.legend(pz, [l.get_label() for l in pz], loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history plot shows characteristic features of training performance over successive epochs.  Accuracy and loss are related, in that a reduction in loss produces an increase in accuracy.  The graph shows characteristic arcs for training and testing accuracy / loss over training time (epochs).\n",
    "\n",
    "The primary measure to improve is *testing accuracy*, because that indicates how well the model generalizes to data it must typically classify.\n",
    "\n",
    "The accuracy curves show that testing accuracy has plateaued (with some variability), while training accuracy increases (but at a slowing rate).  The difference between training and testing accuracy shows overfitting of the model (i.e., the model can memorize what it has seen better than it can generalize the classification rules).\n",
    "\n",
    "We would like a model that *can* overfit (otherwise it might not be large enough to capture the complexity of the data domain), but doesn't.  And then, it is only trained until *test accuracy* peaks.\n",
    "\n",
    "Could the model 100% overfit the data?  The graph doesn't answer definitively yet, but training accuracy seems to be slowing, while training loss is still decreasing (with lots of room to improve – the loss axis does not start at zero).\n",
    "\n",
    "*Note:* The model contains Dropout layers to help prevent overfitting.  What happens to training and testing accuracy when those layers are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_transfer_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following prediction plot functions provide insight into aspects of model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    rSym = ''\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict(x_test[idx:idx+1])[0]\n",
    "        if y_test is not None:\n",
    "            rCorrect = True if cifar10_label(y_test[idx]) == cifar10_label(result) else False\n",
    "            rSym = '✔' if rCorrect else '✘'\n",
    "            correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, cifar10_label(result)))\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    if y_test is not None:\n",
    "        print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_classes_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    rSym = ''\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_classes(x_test[idx:idx+1])[0]\n",
    "        if y_test is not None:\n",
    "            rCorrect = True if cifar10_label(y_test[idx]) == cifar10_label(result) else False\n",
    "            rSym = '✔' if rCorrect else '✘'\n",
    "            correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, cifar10_label(result)))\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    if y_test is not None:\n",
    "        print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_proba_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 2, (2*i) + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_proba(x_test[idx:idx+1])[0] * 100 # prob -> percent\n",
    "        if y_test is not None:\n",
    "            plt.title(\"%s\" % cifar10_label(y_test[idx]))\n",
    "        plt.xlabel(\"#%s\" % idx)\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "        ax = plt.subplot(10, 2, (2*i) + 2)\n",
    "        plt.bar(np.arange(len(result)), result, label='%')\n",
    "        plt.xticks(range(0, len(result) + 1))\n",
    "        ax.set_xticklabels(cifar10_label_names)\n",
    "        plt.title(\"classifier probabilities\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization* by Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra [arXiv (2016)](https://arxiv.org/abs/1610.02391)\n",
    "* https://jacobgil.github.io/deeplearning/class-activation-maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def generate_activation_pattern(model, conv_layer_output, category_idx, image):\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    activation_model = keras.models.Model([model.inputs], [conv_layer_output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as gtape:\n",
    "        conv_output, prediction = activation_model(image)\n",
    "        category_output = prediction[:, category_idx]\n",
    "        grads = gtape.gradient(category_output, conv_output)\n",
    "        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1) * -1.\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap) + epsilon\n",
    "    return(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_plot(model, layer_name, image_data, image_index=None):\n",
    "    (layer_model, conv_layer) = get_model_layer(model, layer_name)\n",
    "    (x_imgs, y_cat) = image_data\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = int(random.uniform(0, x_imgs.shape[0]))\n",
    "    \n",
    "    image = x_imgs[image_index:image_index+1]\n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    plt.subplot(1, num_classes + 2, 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(cifar10_label(y_cat[image_index]))\n",
    "    plt.xlabel(\"#%s\" % image_index)\n",
    "    plt.imshow(image.reshape(32, 32, 3))\n",
    "\n",
    "    result = model.predict(image)[0]\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        activation = generate_activation_pattern(model, conv_layer.output, i, image)\n",
    "        activation = np.copy(activation)\n",
    "        plt.subplot(1, num_classes + 2, i + 2)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(cifar10_label(i))\n",
    "        plt.xlabel(\"(% 3.2f%%)\" % (result[i] * 100.0))\n",
    "        plt.imshow(activation[0])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows what the model thinks is the most likely class for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model_transfer_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the probabilities that the model assigns to each category class, and provides a sense of how confident the network is with its classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model_transfer_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Complete activation plot\n",
    "#activation_plot(model_transfer_cnn, ('vgg16', 'block5_conv3'), (x_test, y_test), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Classifier Model\n",
    "\n",
    "Create a basic CNN (Convolutional Neural Network) based classifier from scratch.\n",
    "\n",
    "We have encountered Conv2D and MaxPooling2D layers previously, but here we see how they are declared.  Conv2D layers specify the number of convolution kernels and their shape.  MaxPooling2D layers specify the size of each pool (i.e., the scaling factors).\n",
    "\n",
    "Notice the total number of parameters (\\~1.25 million) in this smaller network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "def create_cnn_classifier_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_simple_cnn = create_cnn_classifier_model()\n",
    "model_simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model_simple_cnn.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model_simple_cnn.fit(x_train, y_train,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               validation_data=(x_test, y_test),\n",
    "                               shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notable features of the history plot for this model are:\n",
    "\n",
    "* Training accuracy is ~10 percentage points better than the previous model,\n",
    "* test accuracy more closely tracks training accuracy, and\n",
    "* test accuracy shows more variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_simple_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model_simple_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model_simple_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_weights(model_simple_cnn, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_index = cifar10_image_plot()\n",
    "for n in [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)]:\n",
    "    visualize_conv_layer_output(model_simple_cnn, n, image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting aspects of the convolutional layer response for our *model_simple_cnn* model:\n",
    "\n",
    "* There are fewer Conv2D layers in this simple model\n",
    "* Compared to the pre-trained VGG16 convolutional base network, \n",
    "  * the latter levels are the first edge detection kernels, and \n",
    "  * there are no layers with higher-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_response(model_simple_cnn, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows which pixels of the original image contributed the most 'confidence' to the classification categories.\n",
    "\n",
    "The technique is better applied to larger images where the object of interest might be anywhere inside the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)][-1]\n",
    "print(n)\n",
    "for i in range(5):\n",
    "    activation_plot(model_simple_cnn, n, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Models\n",
    "\n",
    "Keras supports a functional interface to take network architectures beyond simply sequential networks.\n",
    "\n",
    "The new layer types are Input and Concatenate; and, there is an explicit Model class.\n",
    "\n",
    "The Input layer is a special layer denoting sources of input from training batches.\n",
    "\n",
    "The Concatenate layer combines multiple inputs (along an axis with the same size) and creates a larger layer incorporating all the input values.\n",
    "\n",
    "Model construction is also different.  Instead of using a `Sequential` model, and `add`ing layers to it:\n",
    "\n",
    "* An explicit Input layer is created, \n",
    "* we pass inputs into the layers explicity,\n",
    "* the output from a layer become input for arbitrary other layers, and finally,\n",
    "* A Model object is created with the source Input layer as inputs and outputs from the final layer.\n",
    "\n",
    "We'll demonstrate by creating a new network which combines the two CNN classifier networks we created previously.\n",
    "\n",
    "*Note:* Network models provided as an argument are changed to be non-trainable (the assumption is that they were already trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def create_combined_classifier_model(trained_model1=None, trained_model2=None):\n",
    "    if trained_model1:\n",
    "        network1 = trained_model1\n",
    "        network1.trainable = False\n",
    "    else:\n",
    "        network1 = create_cnnbase_classifier_model()\n",
    "\n",
    "    if trained_model2:\n",
    "        network2 = trained_model2\n",
    "        network2.trainable = False\n",
    "    else:\n",
    "        network2 = create_cnn_classifier_model()\n",
    "\n",
    "    inputs = Input(shape=(32,32,3), name='cifar10_image')\n",
    "    c1 = network1(inputs)\n",
    "    c2 = network2(inputs)\n",
    "    c = Concatenate()([c1, c2])\n",
    "    x = Dense(512)(c)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes)(x)\n",
    "    outputs = Activation('softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='combined_cnn_classifier')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Pre-Trained Models\n",
    "\n",
    "This version of the combined classifier uses both of the trained networks we created previously.\n",
    "\n",
    "Notice the trainable parameters (~16,000) is very small.  How will this affect training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_combined = create_combined_classifier_model(model_transfer_cnn, model_simple_cnn)\n",
    "model_combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows a graph representation of the layer connections.  Notice how a single input feeds the previously created Sequential networks, their output is combine via Concatenate, and then a classifier network is added on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce number of `epochs` because this network is mostly trained (execpt for the final classifier), and there are few trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 5 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_combined.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model_combined.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like everything we needed to learn was learned in a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an interesting, possibly counter-intuitive, result: combining two weaker networks can create a stronger one.\n",
    "\n",
    "The reason is that the weakness in one model, might be a strength in the other model (each has 'knowledge' that the other doesn't); we just need a layer to discriminate when to trust each model.  At a larger scale (of layers and models) is what is happening at the lower level of the neurons themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_combined.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Sequential Model provides `predict_classes` or `predict_proba`\n",
    "#       Functional API Model does not; because it may have multiple outputs\n",
    "# Using simple `predict` plot instead\n",
    "prediction_plot(model_combined, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combine model improves accuracy by 0.5-2% (with TF 2.0), and takes 1/5<sup>th</sup> of the time to train.\n",
    "\n",
    "#### Training Combining Models\n",
    "\n",
    "This version of the combined classifier uses both network architectures seen previously; except, in this version, the models need to be trained from scratch.  The following cells repeat the previous experiments with this combined classifier.\n",
    "\n",
    "*Spoiler:* The combined network doesn't perform any better than the partially trained one did, but takes much longer to train (more epochs).\n",
    "\n",
    "**Note:** Skip this step during the tutorial, it will cause unecessary delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_combined = create_combined_classifier_model()\n",
    "model_combined.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])\n",
    "history = model_combined.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_combined.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections\n",
    "\n",
    "From previous comparisons of the `visualize_conv_layer_response` plots of the two basic CNN models, it becomes apparent that the pre-trained VGG16 network contains more complex *knowledge* about images: there were more convolutional layers with a greater variety of patterns and features they could represent.\n",
    "\n",
    "In the previous cnnbase_classifier model `model_transfer_cnn`, only the last Conv2D layer fed directly to the classifier, and the feature information contained in the middle layers wasn't directly available to the classifier.\n",
    "\n",
    "Skip Connections are a way to bring lower level feature encodings to higher levels of the network directly.  They are also useful during training very deep networks to deal with the problem of *vanishing gradients*.\n",
    "\n",
    "In the following example, the original CNN base of the pre-trained VGG16 model is decomposed into layered groups, and a new network created that feeds these intermediate layers to the top of the network, where they are concatenated together to perform the final classification.\n",
    "\n",
    "* https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
    "* https://arxiv.org/abs/1608.04117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def create_cnnbase_skipconnected_classifier_model(conv_base=None):\n",
    "    if not conv_base:\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        conv_base.trainable = False\n",
    "\n",
    "    # Split conv_base into groups of CNN layers topped by a MaxPooling2D layer\n",
    "    cb_idxs = [i for (i,l) in enumerate(conv_base.layers) if isinstance(l, keras.layers.MaxPooling2D)]\n",
    "    all_idxs = [-1] + cb_idxs\n",
    "    idx_pairs = [l for l in zip(all_idxs, cb_idxs)]\n",
    "    cb_layers = [conv_base.layers[i+1:j+1] for (i,j) in idx_pairs]\n",
    "\n",
    "    # Dense Pre-Classifier Layers creation function - used repeatedly at multiple network locations\n",
    "    def dense_classes(l):\n",
    "        x = Dense(512)(l)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes)(x)\n",
    "        return x\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3), name='cifar10_image')\n",
    "\n",
    "    # Join split groups into a sequence, but keep track of their outputs to create skip connections\n",
    "    skips = []\n",
    "    inz = inputs\n",
    "    for lz in cb_layers:\n",
    "        m = Sequential()\n",
    "        m.trainable = False\n",
    "        for ls in lz:\n",
    "            m.add(ls)\n",
    "        # inz is the output of model m, but the input for next layer group\n",
    "        inz = m(inz)\n",
    "        skips += [inz]\n",
    "\n",
    "    # Flatten all outputs (which had different dimensions) to Concatenate them on a common axis\n",
    "    flats = [dense_classes(Flatten()(l)) for l in skips]\n",
    "    c = Concatenate()(flats)\n",
    "    x = dense_classes(c)\n",
    "    outputs = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipconnected = create_cnnbase_skipconnected_classifier_model(conv_base)\n",
    "model_skipconnected.summary()\n",
    "keras.utils.plot_model(model_skipconnected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_skipconnected.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])\n",
    "history = model_skipconnected.fit(x_train, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=(x_test, y_test),\n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant improvement over the first pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_skipconnected.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using simple `predict` plot because model uses Functional API\n",
    "prediction_plot(model_skipconnected, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU Example\n",
    "\n",
    "Using multiple GPUs on a single node is a simple way to speed up deep learning.  Keras / TensorFlow support this with a small modification to code.\n",
    "\n",
    "First, determine if multiple GPUs are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_count = len(physical_devices)\n",
    "print(\"GPU count:\", device_count)\n",
    "print(\"GPU devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scaling to `n` GPUs, there is `n *` the available GPU memory, so we can increase the batch_size by `n`.  A larger batch size means that there is more data evaluated by the batch step, which creates a more accurate and representative loss gradient – so we can take a larger corrective step by multiply the learning_rate by `n`.  Because we are learning `n *` more each epoch, we only need `1/n`<sup>th</sup> the number of training epochs.\n",
    "\n",
    "There are additional subtleties and mitigating strategies to be aware of when scaling batch sizes larger.  Some of these are discussed in [Deep Learning at scale: Accurate, Large Mini batch SGD](https://towardsdatascience.com/deep-learning-at-scale-accurate-large-mini-batch-sgd-8207d54bfe02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Multi-GPU Example\n",
    "assert device_count >= 2, \"Two or more GPUs required to demonstrate multi-gpu functionality\"\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "batch_size = device_count * 128 #32\n",
    "epochs = 30 #25 #100\n",
    "epochs = epochs // device_count + 1 #100\n",
    "learning_rate = device_count * 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "use_opt = 'Adam' #'RMSprop'\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = device_count * 1e-3\n",
    "    warmup_epochs = 5\n",
    "    warmup_lr = (epoch + 1) * initial_lr / warmup_epochs\n",
    "    return warmup_lr if epoch <= warmup_epochs else initial_lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "\n",
    "if use_opt == 'Adam':\n",
    "    optimizer = Adam()\n",
    "    callbacks = []\n",
    "else:\n",
    "    optimizer = RMSprop(learning_rate=learning_rate, decay=decay, momentum=0.5)\n",
    "    callbacks = [lr_reducer, lr_scheduler]\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model_multigpu = create_cnnbase_classifier_model()\n",
    "\n",
    "    model_multigpu.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "history = model_multigpu.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True,\n",
    "                             callbacks=callbacks,\n",
    "                             use_multiprocessing=True, workers=4\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_multigpu.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Bios\n",
    "\n",
    "Glendon Holst is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in HPC workflow solutions for deep learning, image processing, and scientific visualization.\n",
    "\n",
    "David R. Pugh is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in Data Science and Machine Learning. David is also a certified Software and Data Carpentry Instructor and Instructor Trainer and is the lead instructor of the Introduction to Data Science Workshop series at KAUST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\n",
    "* https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "* http://yann.lecun.com/exdb/mnist/index.html\n",
    "* https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751 <br/>\n",
    "  https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e <br/>\n",
    "  https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/\n",
    "* https://towardsdatascience.com/deep-learning-at-scale-accurate-large-mini-batch-sgd-8207d54bfe02\n",
    "* https://arxiv.org/abs/1409.1556 <br/>\n",
    "  https://arxiv.org/abs/1610.02391\n",
    "* https://www.kaggle.com/c/digit-recognizer\n",
    "* https://jupyter-notebook.readthedocs.io/en/stable/\n",
    "* https://github.com/kaust-vislab/handson-ml2\n",
    "* https://keras.io/examples/cifar10_cnn/ <br/>\n",
    "  https://keras.io/examples/cifar10_resnet/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
